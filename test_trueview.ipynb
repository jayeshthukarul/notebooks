{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,classification_report\n",
    "from sklearn.metrics import log_loss, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitor</th>\n",
       "      <th>session</th>\n",
       "      <th>browser</th>\n",
       "      <th>sessions_before</th>\n",
       "      <th>sessions_with_affil_before</th>\n",
       "      <th>affil_after_p1_prev_session</th>\n",
       "      <th>sessions_before_with_checkout</th>\n",
       "      <th>sessions_before_with_cart</th>\n",
       "      <th>sessions_before_with_product</th>\n",
       "      <th>daily_sessions</th>\n",
       "      <th>...</th>\n",
       "      <th>entry_home</th>\n",
       "      <th>entry_product</th>\n",
       "      <th>page_depth</th>\n",
       "      <th>first_page_affil</th>\n",
       "      <th>coupon_run_on_checkout</th>\n",
       "      <th>coupon_run_on_cart</th>\n",
       "      <th>coupon_run_on_product</th>\n",
       "      <th>products_viewed</th>\n",
       "      <th>mal_cb</th>\n",
       "      <th>coupon_run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00006362-4996-4898-9aae-21317547d39e</td>\n",
       "      <td>8196a21e-e172-414a-bebe-231bfd8019ab</td>\n",
       "      <td>IE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00006362-4996-4898-9aae-21317547d39e</td>\n",
       "      <td>93660917-7a56-42f8-a59e-d06af115daad</td>\n",
       "      <td>IE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00006362-4996-4898-9aae-21317547d39e</td>\n",
       "      <td>8bf4bde4-6bf3-4fb8-ae03-0e52a9c41238</td>\n",
       "      <td>IE</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000879b-5642-476a-b307-da1736aa0e23</td>\n",
       "      <td>61398de4-3c0b-4630-b7e8-09610d19935b</td>\n",
       "      <td>IE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000879b-5642-476a-b307-da1736aa0e23</td>\n",
       "      <td>62725745-7a89-46c7-a07d-4b745daffd69</td>\n",
       "      <td>IE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                visitor                               session  \\\n",
       "0  00006362-4996-4898-9aae-21317547d39e  8196a21e-e172-414a-bebe-231bfd8019ab   \n",
       "1  00006362-4996-4898-9aae-21317547d39e  93660917-7a56-42f8-a59e-d06af115daad   \n",
       "2  00006362-4996-4898-9aae-21317547d39e  8bf4bde4-6bf3-4fb8-ae03-0e52a9c41238   \n",
       "3  0000879b-5642-476a-b307-da1736aa0e23  61398de4-3c0b-4630-b7e8-09610d19935b   \n",
       "4  0000879b-5642-476a-b307-da1736aa0e23  62725745-7a89-46c7-a07d-4b745daffd69   \n",
       "\n",
       "  browser  sessions_before  sessions_with_affil_before  \\\n",
       "0      IE                0                           0   \n",
       "1      IE                1                           0   \n",
       "2      IE                2                           0   \n",
       "3      IE                0                           0   \n",
       "4      IE                1                           0   \n",
       "\n",
       "   affil_after_p1_prev_session  sessions_before_with_checkout  \\\n",
       "0                            0                              0   \n",
       "1                            0                              0   \n",
       "2                            0                              0   \n",
       "3                            0                              0   \n",
       "4                            0                              0   \n",
       "\n",
       "   sessions_before_with_cart  sessions_before_with_product  daily_sessions  \\\n",
       "0                          0                             0               0   \n",
       "1                          0                             1               0   \n",
       "2                          0                             1               1   \n",
       "3                          0                             0               0   \n",
       "4                          1                             1               1   \n",
       "\n",
       "      ...      entry_home  entry_product  page_depth  first_page_affil  \\\n",
       "0     ...               1              0          14                 0   \n",
       "1     ...               1              0           1                 0   \n",
       "2     ...               1              0           1                 0   \n",
       "3     ...               1              0          34                 0   \n",
       "4     ...               1              0           2                 0   \n",
       "\n",
       "   coupon_run_on_checkout  coupon_run_on_cart  coupon_run_on_product  \\\n",
       "0                       0                   0                      0   \n",
       "1                       0                   0                      0   \n",
       "2                       0                   0                      0   \n",
       "3                       0                   0                      0   \n",
       "4                       0                   0                      0   \n",
       "\n",
       "   products_viewed  mal_cb  coupon_run  \n",
       "0                5       0           0  \n",
       "1                0       0           0  \n",
       "2                0       0           0  \n",
       "3                4       0           0  \n",
       "4                0       0           0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data.to_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coupon_run(x):\n",
    "    x1 = bool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coupon_run(x,y,z):\n",
    "    print(type(x))\n",
    "    print(type(x==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_idx = raw_data[(raw_data['coupon_run_on_cart']==1)|(raw_data['coupon_run_on_checkout']==1)|(raw_data['coupon_run_on_product']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2           6245\n",
       "5            826\n",
       "3            702\n",
       "Edge         518\n",
       "IE           327\n",
       "7              4\n",
       "Vivaldi        1\n",
       "Chromium       1\n",
       "Name: browser, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_idx['browser'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775198"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(raw_data)):\n",
    "    if(i in tst_idx.index):\n",
    "        test_df.append(1)\n",
    "    else:\n",
    "        test_df.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_arr = np.array(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(775198,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['coupon_run'] = test_df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    766574\n",
       "1      8624\n",
       "Name: coupon_run, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['coupon_run'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sessions_before</th>\n",
       "      <th>sessions_with_affil_before</th>\n",
       "      <th>affil_after_p1_prev_session</th>\n",
       "      <th>sessions_before_with_checkout</th>\n",
       "      <th>sessions_before_with_cart</th>\n",
       "      <th>sessions_before_with_product</th>\n",
       "      <th>daily_sessions</th>\n",
       "      <th>trialy_sessions</th>\n",
       "      <th>quintally_sessions</th>\n",
       "      <th>tenally_sessions</th>\n",
       "      <th>coupon_run_prev_ses_checkout_session</th>\n",
       "      <th>coupon_run_prev_ses_cart_session</th>\n",
       "      <th>coupon_run_prev_ses_product_session</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>day_between_previous_session</th>\n",
       "      <th>daily_visitor</th>\n",
       "      <th>trialy_visitor</th>\n",
       "      <th>quintally_visitor</th>\n",
       "      <th>tenally_visitor</th>\n",
       "      <th>previous_session_cart</th>\n",
       "      <th>previous_session_checkout</th>\n",
       "      <th>previous_session_product</th>\n",
       "      <th>entry_home</th>\n",
       "      <th>entry_product</th>\n",
       "      <th>page_depth</th>\n",
       "      <th>first_page_affil</th>\n",
       "      <th>coupon_run_on_checkout</th>\n",
       "      <th>coupon_run_on_cart</th>\n",
       "      <th>coupon_run_on_product</th>\n",
       "      <th>products_viewed</th>\n",
       "      <th>mal_cb</th>\n",
       "      <th>coupon_run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8.624000e+03</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.000000</td>\n",
       "      <td>8624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.827226</td>\n",
       "      <td>1.321197</td>\n",
       "      <td>0.169991</td>\n",
       "      <td>0.800325</td>\n",
       "      <td>1.711155</td>\n",
       "      <td>0.527713</td>\n",
       "      <td>0.439471</td>\n",
       "      <td>0.306238</td>\n",
       "      <td>0.609810</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>0.143785</td>\n",
       "      <td>0.164889</td>\n",
       "      <td>8.719834e+05</td>\n",
       "      <td>1.774003</td>\n",
       "      <td>0.357375</td>\n",
       "      <td>0.152597</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>0.079661</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.483186</td>\n",
       "      <td>0.390770</td>\n",
       "      <td>0.249536</td>\n",
       "      <td>23.048237</td>\n",
       "      <td>0.122565</td>\n",
       "      <td>0.064355</td>\n",
       "      <td>0.455705</td>\n",
       "      <td>0.520524</td>\n",
       "      <td>4.520292</td>\n",
       "      <td>0.085111</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.400316</td>\n",
       "      <td>1.881524</td>\n",
       "      <td>3.990745</td>\n",
       "      <td>0.591877</td>\n",
       "      <td>1.798441</td>\n",
       "      <td>2.809039</td>\n",
       "      <td>0.929460</td>\n",
       "      <td>1.069888</td>\n",
       "      <td>0.983113</td>\n",
       "      <td>1.769312</td>\n",
       "      <td>0.093488</td>\n",
       "      <td>0.547808</td>\n",
       "      <td>0.525752</td>\n",
       "      <td>1.117128e+07</td>\n",
       "      <td>3.828949</td>\n",
       "      <td>0.479254</td>\n",
       "      <td>0.359620</td>\n",
       "      <td>0.246630</td>\n",
       "      <td>0.270784</td>\n",
       "      <td>0.410592</td>\n",
       "      <td>0.197876</td>\n",
       "      <td>0.499746</td>\n",
       "      <td>0.490322</td>\n",
       "      <td>0.432770</td>\n",
       "      <td>21.273613</td>\n",
       "      <td>0.327956</td>\n",
       "      <td>0.245399</td>\n",
       "      <td>0.498063</td>\n",
       "      <td>0.499608</td>\n",
       "      <td>5.976998</td>\n",
       "      <td>0.279063</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.220000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.827496e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.534490e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.070520e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.648551e+08</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sessions_before  sessions_with_affil_before  \\\n",
       "count      8624.000000                 8624.000000   \n",
       "mean          2.625000                    0.827226   \n",
       "std           4.400316                    1.881524   \n",
       "min           0.000000                    0.000000   \n",
       "25%           0.000000                    0.000000   \n",
       "50%           1.000000                    0.000000   \n",
       "75%           3.000000                    1.000000   \n",
       "max          82.000000                   64.000000   \n",
       "\n",
       "       affil_after_p1_prev_session  sessions_before_with_checkout  \\\n",
       "count                  8624.000000                    8624.000000   \n",
       "mean                      1.321197                       0.169991   \n",
       "std                       3.990745                       0.591877   \n",
       "min                       0.000000                       0.000000   \n",
       "25%                       0.000000                       0.000000   \n",
       "50%                       0.000000                       0.000000   \n",
       "75%                       1.000000                       0.000000   \n",
       "max                      94.000000                      10.000000   \n",
       "\n",
       "       sessions_before_with_cart  sessions_before_with_product  \\\n",
       "count                8624.000000                   8624.000000   \n",
       "mean                    0.800325                      1.711155   \n",
       "std                     1.798441                      2.809039   \n",
       "min                     0.000000                      0.000000   \n",
       "25%                     0.000000                      0.000000   \n",
       "50%                     0.000000                      1.000000   \n",
       "75%                     1.000000                      2.000000   \n",
       "max                    31.000000                     35.000000   \n",
       "\n",
       "       daily_sessions  trialy_sessions  quintally_sessions  tenally_sessions  \\\n",
       "count     8624.000000      8624.000000         8624.000000       8624.000000   \n",
       "mean         0.527713         0.439471            0.306238          0.609810   \n",
       "std          0.929460         1.069888            0.983113          1.769312   \n",
       "min          0.000000         0.000000            0.000000          0.000000   \n",
       "25%          0.000000         0.000000            0.000000          0.000000   \n",
       "50%          0.000000         0.000000            0.000000          0.000000   \n",
       "75%          1.000000         1.000000            0.000000          1.000000   \n",
       "max         15.000000        24.000000           53.000000         58.000000   \n",
       "\n",
       "       coupon_run_prev_ses_checkout_session  coupon_run_prev_ses_cart_session  \\\n",
       "count                           8624.000000                       8624.000000   \n",
       "mean                               0.008581                          0.143785   \n",
       "std                                0.093488                          0.547808   \n",
       "min                                0.000000                          0.000000   \n",
       "25%                                0.000000                          0.000000   \n",
       "50%                                0.000000                          0.000000   \n",
       "75%                                0.000000                          0.000000   \n",
       "max                                2.000000                         11.000000   \n",
       "\n",
       "       coupon_run_prev_ses_product_session    time_spent  \\\n",
       "count                          8624.000000  8.624000e+03   \n",
       "mean                              0.164889  8.719834e+05   \n",
       "std                               0.525752  1.117128e+07   \n",
       "min                               0.000000  8.220000e+00   \n",
       "25%                               0.000000  1.827496e+04   \n",
       "50%                               0.000000  3.534490e+04   \n",
       "75%                               0.000000  7.070520e+04   \n",
       "max                               6.000000  6.648551e+08   \n",
       "\n",
       "       day_between_previous_session  daily_visitor  trialy_visitor  \\\n",
       "count                   8624.000000    8624.000000     8624.000000   \n",
       "mean                       1.774003       0.357375        0.152597   \n",
       "std                        3.828949       0.479254        0.359620   \n",
       "min                        0.000000       0.000000        0.000000   \n",
       "25%                        0.000000       0.000000        0.000000   \n",
       "50%                        0.000000       0.000000        0.000000   \n",
       "75%                        2.000000       1.000000        0.000000   \n",
       "max                       29.000000       1.000000        1.000000   \n",
       "\n",
       "       quintally_visitor  tenally_visitor  previous_session_cart  \\\n",
       "count        8624.000000      8624.000000            8624.000000   \n",
       "mean            0.065051         0.079661               0.214634   \n",
       "std             0.246630         0.270784               0.410592   \n",
       "min             0.000000         0.000000               0.000000   \n",
       "25%             0.000000         0.000000               0.000000   \n",
       "50%             0.000000         0.000000               0.000000   \n",
       "75%             0.000000         0.000000               0.000000   \n",
       "max             1.000000         1.000000               1.000000   \n",
       "\n",
       "       previous_session_checkout  previous_session_product   entry_home  \\\n",
       "count                8624.000000               8624.000000  8624.000000   \n",
       "mean                    0.040816                  0.483186     0.390770   \n",
       "std                     0.197876                  0.499746     0.490322   \n",
       "min                     0.000000                  0.000000     0.000000   \n",
       "25%                     0.000000                  0.000000     0.000000   \n",
       "50%                     0.000000                  0.000000     0.000000   \n",
       "75%                     0.000000                  1.000000     1.000000   \n",
       "max                     1.000000                  1.000000     2.000000   \n",
       "\n",
       "       entry_product   page_depth  first_page_affil  coupon_run_on_checkout  \\\n",
       "count    8624.000000  8624.000000       8624.000000             8624.000000   \n",
       "mean        0.249536    23.048237          0.122565                0.064355   \n",
       "std         0.432770    21.273613          0.327956                0.245399   \n",
       "min         0.000000     2.000000          0.000000                0.000000   \n",
       "25%         0.000000     9.000000          0.000000                0.000000   \n",
       "50%         0.000000    17.000000          0.000000                0.000000   \n",
       "75%         0.000000    29.000000          0.000000                0.000000   \n",
       "max         1.000000   238.000000          1.000000                1.000000   \n",
       "\n",
       "       coupon_run_on_cart  coupon_run_on_product  products_viewed  \\\n",
       "count         8624.000000            8624.000000      8624.000000   \n",
       "mean             0.455705               0.520524         4.520292   \n",
       "std              0.498063               0.499608         5.976998   \n",
       "min              0.000000               0.000000         0.000000   \n",
       "25%              0.000000               0.000000         1.000000   \n",
       "50%              0.000000               1.000000         3.000000   \n",
       "75%              1.000000               1.000000         6.000000   \n",
       "max              1.000000               1.000000       131.000000   \n",
       "\n",
       "            mal_cb  coupon_run  \n",
       "count  8624.000000      8624.0  \n",
       "mean      0.085111         1.0  \n",
       "std       0.279063         0.0  \n",
       "min       0.000000         1.0  \n",
       "25%       0.000000         1.0  \n",
       "50%       0.000000         1.0  \n",
       "75%       0.000000         1.0  \n",
       "max       1.000000         1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anlysis on raw data\n",
    "raw_data[raw_data['coupon_run']==1].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sessions_before</th>\n",
       "      <th>sessions_with_affil_before</th>\n",
       "      <th>affil_after_p1_prev_session</th>\n",
       "      <th>sessions_before_with_checkout</th>\n",
       "      <th>sessions_before_with_cart</th>\n",
       "      <th>sessions_before_with_product</th>\n",
       "      <th>daily_sessions</th>\n",
       "      <th>trialy_sessions</th>\n",
       "      <th>quintally_sessions</th>\n",
       "      <th>tenally_sessions</th>\n",
       "      <th>coupon_run_prev_ses_checkout_session</th>\n",
       "      <th>coupon_run_prev_ses_cart_session</th>\n",
       "      <th>coupon_run_prev_ses_product_session</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>day_between_previous_session</th>\n",
       "      <th>daily_visitor</th>\n",
       "      <th>trialy_visitor</th>\n",
       "      <th>quintally_visitor</th>\n",
       "      <th>tenally_visitor</th>\n",
       "      <th>previous_session_cart</th>\n",
       "      <th>previous_session_checkout</th>\n",
       "      <th>previous_session_product</th>\n",
       "      <th>entry_home</th>\n",
       "      <th>entry_product</th>\n",
       "      <th>page_depth</th>\n",
       "      <th>first_page_affil</th>\n",
       "      <th>coupon_run_on_checkout</th>\n",
       "      <th>coupon_run_on_cart</th>\n",
       "      <th>coupon_run_on_product</th>\n",
       "      <th>products_viewed</th>\n",
       "      <th>mal_cb</th>\n",
       "      <th>coupon_run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>7.665740e+05</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.0</td>\n",
       "      <td>766574.0</td>\n",
       "      <td>766574.0</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.000000</td>\n",
       "      <td>766574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.335405</td>\n",
       "      <td>0.108277</td>\n",
       "      <td>0.149303</td>\n",
       "      <td>0.150522</td>\n",
       "      <td>0.571230</td>\n",
       "      <td>1.951174</td>\n",
       "      <td>0.616539</td>\n",
       "      <td>0.592003</td>\n",
       "      <td>0.384331</td>\n",
       "      <td>0.774454</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>2.640935e+06</td>\n",
       "      <td>2.061759</td>\n",
       "      <td>0.345136</td>\n",
       "      <td>0.157099</td>\n",
       "      <td>0.069411</td>\n",
       "      <td>0.096734</td>\n",
       "      <td>0.124655</td>\n",
       "      <td>0.033750</td>\n",
       "      <td>0.456744</td>\n",
       "      <td>0.402159</td>\n",
       "      <td>0.169155</td>\n",
       "      <td>8.251514</td>\n",
       "      <td>0.012039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.869135</td>\n",
       "      <td>0.035747</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.495479</td>\n",
       "      <td>0.731061</td>\n",
       "      <td>1.154949</td>\n",
       "      <td>0.995264</td>\n",
       "      <td>1.881794</td>\n",
       "      <td>5.239577</td>\n",
       "      <td>2.030250</td>\n",
       "      <td>2.732810</td>\n",
       "      <td>1.743419</td>\n",
       "      <td>2.245324</td>\n",
       "      <td>0.043273</td>\n",
       "      <td>0.166194</td>\n",
       "      <td>0.153295</td>\n",
       "      <td>8.056131e+07</td>\n",
       "      <td>4.142459</td>\n",
       "      <td>0.475413</td>\n",
       "      <td>0.363894</td>\n",
       "      <td>0.254153</td>\n",
       "      <td>0.295596</td>\n",
       "      <td>0.330327</td>\n",
       "      <td>0.180586</td>\n",
       "      <td>0.498126</td>\n",
       "      <td>0.491978</td>\n",
       "      <td>0.374889</td>\n",
       "      <td>13.348851</td>\n",
       "      <td>0.109061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.787498</td>\n",
       "      <td>0.185660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.347605e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.181565e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.802645e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>292.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.084348e+10</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2303.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2303.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sessions_before  sessions_with_affil_before  \\\n",
       "count    766574.000000               766574.000000   \n",
       "mean          3.335405                    0.108277   \n",
       "std           8.495479                    0.731061   \n",
       "min           0.000000                    0.000000   \n",
       "25%           0.000000                    0.000000   \n",
       "50%           1.000000                    0.000000   \n",
       "75%           3.000000                    0.000000   \n",
       "max         292.000000                   65.000000   \n",
       "\n",
       "       affil_after_p1_prev_session  sessions_before_with_checkout  \\\n",
       "count                766574.000000                  766574.000000   \n",
       "mean                      0.149303                       0.150522   \n",
       "std                       1.154949                       0.995264   \n",
       "min                       0.000000                       0.000000   \n",
       "25%                       0.000000                       0.000000   \n",
       "50%                       0.000000                       0.000000   \n",
       "75%                       0.000000                       0.000000   \n",
       "max                     117.000000                      64.000000   \n",
       "\n",
       "       sessions_before_with_cart  sessions_before_with_product  \\\n",
       "count              766574.000000                 766574.000000   \n",
       "mean                    0.571230                      1.951174   \n",
       "std                     1.881794                      5.239577   \n",
       "min                     0.000000                      0.000000   \n",
       "25%                     0.000000                      0.000000   \n",
       "50%                     0.000000                      1.000000   \n",
       "75%                     0.000000                      2.000000   \n",
       "max                    81.000000                    244.000000   \n",
       "\n",
       "       daily_sessions  trialy_sessions  quintally_sessions  tenally_sessions  \\\n",
       "count   766574.000000    766574.000000       766574.000000     766574.000000   \n",
       "mean         0.616539         0.592003            0.384331          0.774454   \n",
       "std          2.030250         2.732810            1.743419          2.245324   \n",
       "min          0.000000         0.000000            0.000000          0.000000   \n",
       "25%          0.000000         0.000000            0.000000          0.000000   \n",
       "50%          0.000000         0.000000            0.000000          0.000000   \n",
       "75%          1.000000         1.000000            0.000000          1.000000   \n",
       "max         47.000000        95.000000           95.000000         64.000000   \n",
       "\n",
       "       coupon_run_prev_ses_checkout_session  coupon_run_prev_ses_cart_session  \\\n",
       "count                         766574.000000                     766574.000000   \n",
       "mean                               0.001818                          0.015278   \n",
       "std                                0.043273                          0.166194   \n",
       "min                                0.000000                          0.000000   \n",
       "25%                                0.000000                          0.000000   \n",
       "50%                                0.000000                          0.000000   \n",
       "75%                                0.000000                          0.000000   \n",
       "max                                2.000000                          9.000000   \n",
       "\n",
       "       coupon_run_prev_ses_product_session    time_spent  \\\n",
       "count                        766574.000000  7.665740e+05   \n",
       "mean                              0.014938  2.640935e+06   \n",
       "std                               0.153295  8.056131e+07   \n",
       "min                               0.000000  6.000000e+00   \n",
       "25%                               0.000000  1.347605e+04   \n",
       "50%                               0.000000  3.181565e+04   \n",
       "75%                               0.000000  6.802645e+04   \n",
       "max                               7.000000  6.084348e+10   \n",
       "\n",
       "       day_between_previous_session  daily_visitor  trialy_visitor  \\\n",
       "count                 766574.000000  766574.000000   766574.000000   \n",
       "mean                       2.061759       0.345136        0.157099   \n",
       "std                        4.142459       0.475413        0.363894   \n",
       "min                        0.000000       0.000000        0.000000   \n",
       "25%                        0.000000       0.000000        0.000000   \n",
       "50%                        0.000000       0.000000        0.000000   \n",
       "75%                        2.000000       1.000000        0.000000   \n",
       "max                       29.000000       1.000000        1.000000   \n",
       "\n",
       "       quintally_visitor  tenally_visitor  previous_session_cart  \\\n",
       "count      766574.000000    766574.000000          766574.000000   \n",
       "mean            0.069411         0.096734               0.124655   \n",
       "std             0.254153         0.295596               0.330327   \n",
       "min             0.000000         0.000000               0.000000   \n",
       "25%             0.000000         0.000000               0.000000   \n",
       "50%             0.000000         0.000000               0.000000   \n",
       "75%             0.000000         0.000000               0.000000   \n",
       "max             1.000000         1.000000               1.000000   \n",
       "\n",
       "       previous_session_checkout  previous_session_product     entry_home  \\\n",
       "count              766574.000000             766574.000000  766574.000000   \n",
       "mean                    0.033750                  0.456744       0.402159   \n",
       "std                     0.180586                  0.498126       0.491978   \n",
       "min                     0.000000                  0.000000       0.000000   \n",
       "25%                     0.000000                  0.000000       0.000000   \n",
       "50%                     0.000000                  0.000000       0.000000   \n",
       "75%                     0.000000                  1.000000       1.000000   \n",
       "max                     1.000000                  1.000000       5.000000   \n",
       "\n",
       "       entry_product     page_depth  first_page_affil  coupon_run_on_checkout  \\\n",
       "count  766574.000000  766574.000000     766574.000000                766574.0   \n",
       "mean        0.169155       8.251514          0.012039                     0.0   \n",
       "std         0.374889      13.348851          0.109061                     0.0   \n",
       "min         0.000000       1.000000          0.000000                     0.0   \n",
       "25%         0.000000       2.000000          0.000000                     0.0   \n",
       "50%         0.000000       4.000000          0.000000                     0.0   \n",
       "75%         0.000000      10.000000          0.000000                     0.0   \n",
       "max         1.000000    2303.000000          1.000000                     0.0   \n",
       "\n",
       "       coupon_run_on_cart  coupon_run_on_product  products_viewed  \\\n",
       "count            766574.0               766574.0    766574.000000   \n",
       "mean                  0.0                    0.0         1.869135   \n",
       "std                   0.0                    0.0         7.787498   \n",
       "min                   0.0                    0.0         0.000000   \n",
       "25%                   0.0                    0.0         0.000000   \n",
       "50%                   0.0                    0.0         1.000000   \n",
       "75%                   0.0                    0.0         2.000000   \n",
       "max                   0.0                    0.0      2303.000000   \n",
       "\n",
       "              mal_cb  coupon_run  \n",
       "count  766574.000000    766574.0  \n",
       "mean        0.035747         0.0  \n",
       "std         0.185660         0.0  \n",
       "min         0.000000         0.0  \n",
       "25%         0.000000         0.0  \n",
       "50%         0.000000         0.0  \n",
       "75%         0.000000         0.0  \n",
       "max         1.000000         0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[raw_data['coupon_run']==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = raw_data.drop(['visitor','session','browser','coupon_run_on_cart','coupon_run_on_checkout','coupon_run_on_product'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(775198, 29)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of variance matrix =  (28, 28)\n"
     ]
    }
   ],
   "source": [
    "# PCA visualization\n",
    "sample_data = np.square(X)\n",
    "\n",
    "# matrix multiplication using numpy\n",
    "covar_matrix = np.matmul(sample_data.T , sample_data)\n",
    "\n",
    "print ( \"The shape of variance matrix = \", covar_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of eigen vectors =  (28, 2)\n",
      "Updated shape of eigen vectors =  (2, 28)\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import eigh \n",
    "\n",
    "# the parameter 'eigvals' is defined (low value to heigh value) \n",
    "# eigh function will return the eigen values in asending order\n",
    "# this code generates only the top 2 (782 and 783) eigenvalues.\n",
    "values, vectors = eigh(covar_matrix, eigvals=(26,27))\n",
    "\n",
    "print(\"Shape of eigen vectors = \",vectors.shape)\n",
    "# converting the eigen vectors into (2,d) shape for easyness of further computations\n",
    "vectors = vectors.T\n",
    "\n",
    "print(\"Updated shape of eigen vectors = \",vectors.shape)\n",
    "# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n",
    "# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " resultanat new data points' shape  (2, 28) X (28, 775198)  =  (2, 775198)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "new_coordinates = np.matmul(vectors, sample_data.T)\n",
    "\n",
    "print (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1st_principal  2nd_principal  label\n",
      "0       2.415294  -3.299780e+09    0.0\n",
      "1      -6.853327  -1.631240e+08    0.0\n",
      "2      -0.045876  -1.405647e+08    0.0\n",
      "3      13.982148  -1.303875e+10    0.0\n",
      "4      -0.098539  -2.421758e+08    0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "labels=Y\n",
    "# appending label to the 2d projected data\n",
    "new_coordinates = np.vstack((new_coordinates, labels)).T\n",
    "\n",
    "# creating a new data frame for ploting the labeled points.\n",
    "dataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py:230: UserWarning: The `size` paramter has been renamed to `height`; please update your code.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 483.875x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "sn.FacetGrid(dataframe, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(775198, 29)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_data.iloc[:,:final_data.shape[1]-1]\n",
    "Y = final_data.iloc[:,final_data.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620158"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train test split\n",
    "int(0.8*len(final_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = final_data[:620157]\n",
    "test = final_data[620157:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['coupon_run'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['coupon_run'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.iloc[:,:train.shape[1]-1]\n",
    "x_test = test.iloc[:,:test.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.iloc[:,train.shape[1]-1]\n",
    "y_test = test.iloc[:,test.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "optimal_lr = LogisticRegression(class_weight='balanced')\n",
    "lp = optimal_lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prd = optimal_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 85.81342999593656\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, log_prd) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92    153232\n",
      "           1       0.06      0.73      0.11      1809\n",
      "\n",
      "   micro avg       0.86      0.86      0.86    155041\n",
      "   macro avg       0.53      0.80      0.52    155041\n",
      "weighted avg       0.99      0.86      0.91    155041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,log_prd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "xgb_model = xgb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_prd = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 98.84933662708573\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    153232\n",
      "           1       0.70      0.02      0.05      1809\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    155041\n",
      "   macro avg       0.84      0.51      0.52    155041\n",
      "weighted avg       0.99      0.99      0.98    155041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, xgb_prd) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,xgb_prd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count_Normal_transacation = len(final_data[final_data[\"coupon_run\"]==0]) # normal transaction are repersented by 0\n",
    "Count_Fraud_transacation = len(final_data[final_data[\"coupon_run\"]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775198"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(normal_indices,fraud_indices,times):#times denote the normal data = times*fraud data\n",
    "    Normal_indices_undersample = np.array(np.random.choice(normal_indices,(times*Count_Fraud_transacation),replace=False))\n",
    "    undersample_data= np.concatenate([fraud_indices,Normal_indices_undersample])\n",
    "    undersample_data = final_data.iloc[undersample_data,:]\n",
    "    \n",
    "    print(\"the normal transacation proportion is :\",len(undersample_data[undersample_data.coupon_run==0])/len(undersample_data))\n",
    "    print(\"the fraud transacation proportion is :\",len(undersample_data[undersample_data.coupon_run==1])/len(undersample_data))\n",
    "    print(\"total number of record in resampled data is:\",len(undersample_data))\n",
    "    return(undersample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_indices = np.array(final_data[final_data.coupon_run==0].index)\n",
    "abnormal_indices = np.array(final_data[final_data.coupon_run==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.5\n",
      "the fraud transacation proportion is : 0.5\n",
      "total number of record in resampled data is: 17248\n"
     ]
    }
   ],
   "source": [
    "Undersample_data = undersample(normal_indices,abnormal_indices,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_undersample = Undersample_data.iloc[:,:Undersample_data.shape[1]-1]\n",
    "\n",
    "Y_undersample = Undersample_data.iloc[:,Undersample_data.shape[1]-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_undersample = np.array(X_undersample)\n",
    "Y_undersample = np.array(Y_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_undersample, Y_undersample, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 50.08695652173913\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67      2592\n",
      "           1       0.00      0.00      0.00      2583\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      5175\n",
      "   macro avg       0.25      0.50      0.33      5175\n",
      "weighted avg       0.25      0.50      0.33      5175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "optimal_lr_under = LogisticRegression(class_weight='balanced')\n",
    "lp = optimal_lr_under.fit(X_train_under, y_train_under)\n",
    "\n",
    "log_prd_undersmp = optimal_lr_under.predict(X_test_under)\n",
    "\n",
    "acc = accuracy_score(y_test_under, log_prd_undersmp) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_under,log_prd_undersmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 98.83321186008862\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    153232\n",
      "           1       0.00      0.00      0.00      1809\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    155041\n",
      "   macro avg       0.49      0.50      0.50    155041\n",
      "weighted avg       0.98      0.99      0.98    155041\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "xgb_model_under = xgb.XGBClassifier()\n",
    "\n",
    "xgb_model_under = xgb_model_under.fit(X_train_under, y_train_under)\n",
    "\n",
    "\n",
    "xgb_prd_undersmpl = xgb_model_under.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, xgb_prd_undersmpl) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,xgb_prd_undersmpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using imblearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01112489970304361"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data[final_data['coupon_run']==1])/len(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(processed_data, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542638, 31)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_imb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(ratio='minority')\n",
    "X_sm, y_sm = smote.fit_sample(X_train_imb,y_train_imb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_sm, y_sm, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "scal = StandardScaler()\n",
    "X_train_smote = scal.fit_transform(X_train_smote)\n",
    "X_test_smote = scal.transform(X_test_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x_test_imb = scal.transform(X_test_imb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "optimal_nb_sm = BernoulliNB(alpha=100)\n",
    "lp = optimal_nb_sm.fit(X_train_smote, y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 64.81381148950808\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.65      0.78    229969\n",
      "           1       0.02      0.66      0.04      2591\n",
      "\n",
      "   micro avg       0.65      0.65      0.65    232560\n",
      "   macro avg       0.51      0.65      0.41    232560\n",
      "weighted avg       0.98      0.65      0.78    232560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_prd_sm = optimal_nb_sm.predict(x_test_imb)\n",
    "\n",
    "acc = accuracy_score(y_test_imb, nb_prd_sm) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_imb,nb_prd_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 65.88189888074103\n",
      "False positive rate: 35.19822236910192\n",
      "True negative rate: 64.80177763089809\n",
      "False negative rate: 34.118101119258974\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y_test_np= np.asarray(y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mat = confusion_matrix(y_test_imb,nb_prd_sm)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_imb,nb_prd_sm).ravel()\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,cmap='RdYlBu_r',\n",
    "            xticklabels=['0','1'], yticklabels=['0','1'])\n",
    "plt.title(\"confusion Matrix\")\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "\n",
    "print(\"True positive rate:\",(tp/(tp+fn))*100)\n",
    "print(\"False positive rate:\",(fp/(tn+fp))*100)\n",
    "print(\"True negative rate:\",(tn/(tn+fp))*100)\n",
    "print(\"False negative rate:\",(fn/(tp+fn))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "optimal_lr_sm = LogisticRegression(C=100,class_weight='balanced')\n",
    "lp = optimal_lr_sm.fit(X_train_smote, y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 95.70734434124527\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98    229969\n",
      "           1       0.02      0.07      0.03      2591\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    232560\n",
      "   macro avg       0.51      0.52      0.51    232560\n",
      "weighted avg       0.98      0.96      0.97    232560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_prd_sm = optimal_lr_sm.predict(x_test_imb)\n",
    "\n",
    "acc = accuracy_score(y_test_imb, log_prd_sm) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_imb,log_prd_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 6.869934388267078\n",
      "False positive rate: 3.2917480182111505\n",
      "True negative rate: 96.70825198178885\n",
      "False negative rate: 93.13006561173293\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAEWCAYAAADmTBXNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3XecH1W9//HXe3ezaZRA6CkkQEBCrqB0KQZCCUXCvYAmtCggIqDo76KAooCCF9ELV0TASEITCQgoQQIhUqWkQWihmBACBAjpkEqyu5/fHzO7fLNs+e5kv9u+7+fjMY+dOefMmTMpnz1zzhRFBGZm1jQlrd0AM7P2yMHTzCwDB08zswwcPM3MMnDwNDPLwMHTzCwDB88OTImbJS2RNGU96jlA0pvN2bbWIOlGST9r7XZYxyDf59lxSToAuBPYKSJWtHZ7apPUD3gbmB4RX85J3wz4APggIvrlUc83gTMiYv+CNNSsDu55dmzbAnPaYuCspbukQTnbJ5IE1WYjqbQ56zNz8GwjJPWRdJ+kBZIWSbouTS+RdLGkdyTNl3SbpI3TvH6SQtJISe9KWijpp2ne6cBNwL6Slku6TNI3JT1d67ghaYd0/UhJr0laJul9Seen6YMlzc3ZZ2dJT0haKmmGpGNy8m6R9AdJD6b1TJa0fSOnfzswMmf7VOC2Wu28UNJbaZ2vSfrP6rYAN+ac59KcdtwgabykFcBBadrlaf4FkiZJKku3v5ueS5c8/rrMICK8tPIClAIvAdcA3YEuwP5p3mnALGA7YAPgPuD2NK8fEMCfgK7ArsCnwM5p/jeBp3OOs852mhbADun6h8AB6fomwJfT9cHA3HS9U9qenwDlwMHAMpKhAYBbgMXAXkAZcAcwtp7zrm5/P+C99M9hZ+BN4BCSXnN12ROAbUh+4X8DWAFs3cB53QJ8DOyX7tMlTbs8zS8BngIuBQYAS4Avtfa/BS/tZ3HPs23YiyQw/CgiVkTE6oio7iGeBFwdEbMjYjlwETC8useUuiwiVkXESyRBeNeM7VgLDJS0UUQsiYgX6iizD0kQvzIi1kTEY8A/gBE5Ze6LiCkRUUESPHdr5Lhz+SxgjqRWrxMgIv4aER9ERFVE3AXMJPlza8j9EfFMus/qWvVVkfRwvw+MA66KiOmN1GdWw8GzbegDvJMGm9q2Ad7J2X6HpEe3ZU7avJz1lSTBLYvjgCOBdyQ9KWnfetrzXhp8ctvUaz3bcxtJD3IE8OfamZJOlfRiOlSwFBgEbNZIne81lBkRc4DHSXq+f8ijjWY1HDzbhveAvrV6k9U+IJn4qdYXqAA+ynCcFUC36g1JW+VmRsTUiBgGbAH8Hbi7nvb0kZT7b6cv8H6G9uS6FzgKmB0Rub8skLQtydDEuUDPiOgBvAqouun11NngrSSSjgT2BR4FfpO96VaMHDzbhikk441XSuouqYuk/dK8O4EfSuovaQPgV8Bd9fRSG/MSsIuk3dKJkUurMySVSzpJ0sYRsRb4BKiso47JJEH4x5I6SRoMfA0Ym6E9NSK5I+Bg4Iw6sruTBMIFaVu/RdLzrPYR0FtSeb7HS2+HGp0ebyTwtTSYmuXFwbMNiIhKkgC0A/AuyRjgN9LsMSSz0U+R3L6zGvhexuP8G/gF8E+SMcOnaxU5BZgj6RPgLODkOupYAxwDHAEsBK4HTo2IN7K0qVbd0yLirTrSXwP+F3iOJFD+B/BMTpHHgBnAPEkL8zzcKJIx0fERsQg4HbhJUs/1OQcrHr5J3swsA/c8zcwycPA0M8vAwdPMLAMHTzOzDOq6r7BN+It28kxWO3L1fj9s7SZYBtOePkuNl6pfU/6fnhhvrtex2hr3PM3MMmizPU8za/tKirj75eBpZpmVFXEEKeJTN7P15Z6nmVkGJR1qCqhpHDzNLDP3PM3MMnDwNDPLwMHTzCwDz7abmWXgnqeZWQYOnmZmGUjFe6+Sg6eZZeaep5lZBp4wMjPLwD1PM7MMHDzNzDJw8DQzy6CYg2cRn7qZra+SkvyXxkjqI+lxSa9LmiHpvDR9U0kTJc1Mf26SpkvStZJmSXpZ0pdz6hqZlp8paWRO+u6SXkn3uVbpvVb1HaPBc2/6H5eZWaKsNP8lDxXAf0fEzsA+wDmSBgIXAo9GxADg0XQb4AhgQLqcCdwASSAELgH2BvYCLskJhjekZav3G5qm13eMejl4mllmzdnzjIgPI+KFdH0Z8DrQCxgG3JoWuxU4Nl0fBtwWiUlAD0lbA4cDEyNicUQsASYCQ9O8jSLiuYgI4LZaddV1jPrPvfFTMjOrW1OCp6QzJU3LWc6sr15J/YAvAZOBLSPiQ0gCLLBFWqwX8F7ObnPTtIbS59aRTgPHqJcnjMwss5ImvEo+IkYBoxorJ2kD4F7gBxHxSQOPgNaVERnSM3HP08wyU6nyXvKqT+pEEjjviIj70uSP0ktu0p/z0/S5QJ+c3XsDHzSS3ruO9IaOUS8HTzPLrKSsJO+lMenM92jg9Yi4OidrHFA9Yz4SuD8n/dR01n0f4OP0knsCcJikTdKJosOACWneMkn7pMc6tVZddR2jXr5sN7PM8u1R5mk/4BTgFUkvpmk/Aa4E7pZ0OvAucEKaNx44EpgFrAS+BRARiyX9EpialvtFRCxO178L3AJ0BR5KFxo4Rr0cPM0sMzXj5zMj4mnqHpcEGFJH+QDOqaeuMcCYOtKnAYPqSF9U1zEa4uBpZpk1c8+zXXHwNLPMmjLb3tE4eJpZZs152d7eOHiaWWYlnfJ77rIjcvA0s8w85mlmloEv283MMnDwNDPLwJftZmYZ+FYlM7MMSjoV7+sxHDzNLDOPeZqZZeAxTzOzDFTEn8908DSzzHzZbmaWQaknjMzMms5jnmZmGfiy3cwsC/c8zcyazj1PM7MsSj1hZGbWZPJsu5lZBr5sNzPLwBNG1phuvbdi39uuostWmxFVVbw16m7evPY2drvqx/T62kFUrVnL8rfeZdK3LmLtx8vY6pCvsNuV/01JeSeq1qxl+o9+w0ePTwJg8EM30XXrzVFZKQv+9TzTzrmMqKqixxd3Yq8bL6Nsg26smPM+z5x0PhXLVlDSqRN7/vEyeu4xiKgKnj/vCuY/OaWV/0Tary236M5lFx9Mz027URXB38a9zti/vlKTf/KIXfnBOfsy5Khb+Pjj1WzbtweX/GQwX9hxc67/0xT+fOdLAJSXl/Kn64bRqbyE0tISHn18NqPGTGut02oVnjCyRlVVVPLCf1/JkumvUbZBd4Y+fy8fTnyGeROf4aWL/peorGS3K89nl4u+w4sX/pZPFy7hya99l1UfzmfjXQZw0ITR/L33gQA8/fXzqFi2AoD977mWvicM5Z27xrP3TVcw/fxfM/+pqWz3reMY+KMzePnnv2P7b58AwPgvHkPnzTfloIf+xMN7Hg8Rrfbn0Z5VVAbXXPccb/57Id26duL2Mccxeepc3p6zhC236M7ee/Tmw3nLasp/8slqfvt/zzD4wP7r1LNmTSVnnTeOVasqKC0tYfQNw3h28ru8OmN+S59S6yniCaOCnbmkL0i6QNK1kn6Xru9cqOMV2up5C1gy/TUAKpav4JPXZ9Ot15bMm/gMUVkJwMJJL9Kt91YALHnxdVZ9mPwn+njGTEq7lFNS3inZPw2cKiujtLwTkQbBjXbqz/ynpgIwb+Iz9DnuMAA2HrgDHz2a9Fo/XbCYNUuX0XOPQS1x2h3SokUrefPfCwFYuWotc+YsYYvNugPw/773Fa69YdI6v5eWLF3Na28soKKi6nN1rVpVAUBZWQllpSVF9/tMJcp76WgKEjwlXQCMBQRMAaam63dKurAQx2xJ3bftxSZf2pmFk19aJ337047jg4ee+lz5PscdzpLpr1O1Zm1N2kEP38Rx859l7bIVvHfPBACWvvpveh0zBIC+JwylW5+tAVjy0hv0HjYElZbSvV9vNt19l5o8Wz9bb7UhO+24Ga++9hEH7rct8xeuZOasRXnvX1Ii7rj5eCY+MJLJ0+Yy47Ui6nUClJfkv3QwhTqj04E9I+LKiPhzulwJ7JXm1UnSmZKmSZr2GEsL1LT1U9a9Gwfcey3P/+BXNT1IgF1+chZVFZXMuWPcOuU3HrgDu/36fKZ85+frpD8+9Azu23p/SjqXs+XB+wAw+bSfsuM5JzJ02r2UbdidqjVrAJg95l5Wzp3H0Gn3svv//YSFz06nqqKywGfa8XXtWsZVVxzG//7uWSoqg9NGfpkbb5rapDqqqoKTvnUPR/7X7eyy8xZs33+TArW2bSrmnmehxjyrgG2Ad2qlb53m1SkiRgGjAP6indrcBZDKyjjg3muZc8cDzP3bxJr0/qceS6+jB/PokG+uU75rry054G/X8dypF7B89nufq6/q0zW8P+4xeg8bwrx/Pssnb87m8cOT3y0bDuhHr6MGAxCVlbzw//6nZr9Dn7mTZTPnNPfpFZXS0hKuuvxwHn5kJo8/9Tbbb7cp22y9EXfekowvb7F5d+4Ycxwjv30fixavarS+5cvX8Pz0D9h3n7689faSQje/7fBse7P7AfCopJlAddToC+wAnFugYxbcPqOv4OPXZ/PGNbfUpG19+AEMvODb/POrJ1O5anVNeqeNN2Twg6N46aKrWfjsCzXpZd27UbZhd1bPW4BKS9nmyK+y4F/JDG3nzTfl0wWLQWLQxd9l5o1jASjt2gUkKleuYqtDvkJUVPLJ62+1zEl3UD+/6Ku8/c4S7rjrZQDemr2Yw752a03+uL+exCln3MvHH6+urwp69OhCRUUVy5evoXN5KXvt0Ztb75he8La3KX4ZcvOKiIcl7Uhymd6LZLxzLjA1Itrl9ebm++1O/1OPZcnLb3LE9L8D8NJPrmb3ay+mpHM5B0+8GYCFk15i6ncvYcdzT2bDHfoy6GdnM+hnZwPw2GGnIYmvjruBks7lqLSEjx6bVBMk+404mgHnnAjAe/dNZPbN9wLQZYueHDRhNFFVxar3P+LZU37c0qffoez6xa04auhOzJy1iDtuPh6A6/84hWcmvVtn+Z6bduW2m46je/dyoioYccJ/8PWT72Kznt247KcHU1IiSkrExMfe4uln666jo2rOV9JJGgMcDcyPiEE56d8j6XRVAA9GxI/T9ItIhgErge9HxIQ0fSjwO6AUuCkdMkRSf5K5mE2BF4BTImKNpM7AbcDuwCLgGxExp9H2RhudHmyLl+1Wv6v3+2FrN8EymPb0WesV/dbeNCLv/6edzrizwWNJOhBYDtxWHTwlHQT8FDgqIj6VtEVEzJc0ELiTpIO2DfBPYMe0qn8Dh5J22IAREfGapLuB+yJirKQbgZci4gZJZwNfjIizJA0H/jMivtHY+RRvn9vM1ltzThhFxFPA4lrJ3wWujIhP0zLVtzMMA8ZGxKcR8TYwiySQ7gXMiojZEbGGpKc5TJKAg4F70v1vBY7Nqat6zOYeYEhavkEOnmaWXWlJ3kvu3TTpcmYeR9gROEDSZElPStozTe/FZ/MpkPQyezWQ3hNYGhEVtdLXqSvN/zgt3yA/YWRm2TVhzDP3bpomKAM2AfYB9gTulrQdyTzK5w5B3R3CaKA8jeQ12DAzs0xa4P7NuSTjlAFMkVQFbJam98kp1xv4IF2vK30h0ENSWdq7zC1fXddcSWXAxnx++OBzfNluZtmVKv8lm7+TjFWS3sFTThIIxwHDJXVOZ9EH8NnTjAMk9ZdUDgwHxqXB93Hg+LTekcD96fq4dJs0/7HIYybdPU8zy6w5X4Ys6U5gMLCZpLnAJcAYYIykV4E1wMg0sM1IZ89fI7mF6Zzq2yAlnQtMILlVaUxEzEgPcQEwVtLlwHRgdJo+Grhd0iySHufwfNrr4Glm2TXjTfIRMaKerJPrKX8FcEUd6eOB8XWkzyaZja+dvho4oUmNxcHTzNaHnzAyM8vAwdPMLIMO+LakfDl4mll27nmamWVQVrwhpHjP3MzWn3uenydp04Z2jIhG78A3sw7OwbNOz9PwM6HbFaRFZtZ+OHh+XkT0ry/PzAzwbHtD0vfanQT0j4hfSuoLbBURUwreOjNr24p4wiifPvf1wL7Aien2MuAPBWuRmbUbUkneS0eTz6+NvSPiy5KmA0TEkvRtJWZW7Dzm2aC1kkpJXw4qaXMa+HywmRURB88GXQv8DdhS0hUk77u7uKCtMrP2wRNG9YuIOyQ9DwxJk46NiNcL2ywzaxfc82xUN5IXiwbQtXDNMbN2xbPt9ZP0c5LPcm5K8u2QmyX5st3Mkp5nvksHk8+vjRHAl9K3LSPpSuAF4PJCNszM2oEOGBTzlU/wnAN0AVan252BtwrVIDNrRzxh9HmSfk8yxvkpyceWJqbbhwJPt0zzzKxNc8+zTtPSn8+T3KpU7YmCtcbM2pcinjBq6MUgt7ZkQ8ysHeqAj13mK58XgwwA/gcYSDL2CUBE+JV0ZsWuiINnPmd+M3ADyYflDwJuA24vZKPMrJ0o4luV8jmjrhHxKKCIeCciLgUOLmyzzKxdkPJfOph8RntXK3mf1ExJ5wLvA1sUtllm1i4U8WV7PsHzBySPZ34f+CVJr3NkIRtlZu1EqWfb6xURU9PV5cC3CtscM2tX3PP8PEkPkL7Dsy4RcUxBWmRm7YeDZ51+22KtMLP2qQNOBOWr3l8bEfFkQ0tLNtLM2iiV5L80VpU0RtJ8Sa/mpP1G0huSXpb0N0k9cvIukjRL0puSDs9JH5qmzZJ0YU56f0mTJc2UdFf154QkdU63Z6X5/fI59eLtc5vZ+mve+zxvAYbWSpsIDIqILwL/Bi4CkDQQGA7sku5zvaTS9JNBfwCOIHmwZ0RaFuDXwDURMQBYApyepp8OLImIHYBr0nKNn3o+hczM6qKSsryXxkTEU8DiWmmPRERFujkJ6J2uDwPGRsSnEfE2MAvYK11mRcTsiFgDjAWGpZ9QPxi4J93/VuDYnLqqH0e/BxiSlm+Qg6eZZdeMl+15OA14KF3vBbyXkzc3TasvvSewNCcQV6evU1ea/3FavkGebTez7JowYSTpTODMnKRRETEqz31/SvKI+B3VSXUUC+ruEEYD5Ruqq0H5zLb/F7AV8Od0ewTJC5LNrNg14Zn1NFDmFSxzSRoJHA0MiYjqoDYX6JNTrDfwQbpeV/pCoIeksrR3mVu+uq65ksqAjak1fFCXhl5J92Ta8F9GxIE5WQ9Ieqqxis2sCBT4Pk9JQ4ELgK9GxMqcrHHAXyRdDWwDDACmkPQiB0jqT/Io+XDgxIgISY+TfDp9LMlTkvfn1DUSeC7NfywnSNcrn2erNpe0XUTMTk+mP7B5HvuZWUeXx0RQviTdCQwGNpM0F7iEZHa9MzAxncOZFBFnRcQMSXcDr5Fczp8TEZVpPecCE0i++DsmImakh7gAGCvpcmA6MDpNHw3cLmkWSY9zeD7tzefMfwg8IWl2ut0P+E4+lZtZB9eMr5qLiBF1JI+uI626/BXAFXWkjwfG15E+m2Q2vnb6auCEJjWW/J5tfzh9IfIX0qQ3IuLTph7IzDqgIn48M5/vtncDfgScGxEvAX0lHV3wlplZ21fE7/PM903ya4B90+25+JvtZgYtfZ9nm5LPmOf2EfENSSMAImJVPnffr69rj7yo0IewZlS1cGXjhazj6YBBMV/5BM81krqS3jQqaXuSb7mbWZGryiuEJDpamM3nzC8FHgb6SLoD2A+/FNnMgKC0tZvQavKZbX9E0vPAPiQ3oJ4XEQsL3jIza/MimtCf7GBzRvl8t/3RiBgCPFhHmpkVsehwF+P5a+jFIF1IPvy2maRN+Oz3xkYkj0OZWZGrakrPs4NpqOf5HZIvZ24DPM9nwfMTkpeNmlmR85hnHSLid8DvJH0vIn7fgm0ys3aiKor308P59Lmran03ZBNJZxewTWbWTlRRkvfS0eRzRt+OiKXVGxGxBPh24ZpkZu1FREneS0eTT5+7RJKq32+XfmCpvLDNMrP2wGOeDZsA3C3pRpKnjM4iuWnezIpcR+xR5iuf4HkBycz7d0lm3B8Bbipko8ysfags4gmjfJ4wqgJuSBczsxq+bK+DpLsj4uuSXqGOL8mlH6E3syLmy/a6nZf+9IuPzaxOfjyzDhHxYfrznZZrjpm1J+551kHSMhr48HtEbFSQFplZu1HlMc/Pi4gNAST9ApgH3E4y234SsGGLtM7M2rRifjwznzM/PCL2ztm+QdJk4KoCtcnM2olivmzP58wrJZ0kqVRSiaSTgMpCN8zM2r4qSvNeOpp8gueJwNeBj9LlhDTNzIqcn21vQETMAYYVvilm1t4U861KjZ65pB0lPSrp1XT7i5IuLnzTzKytq4rIe+lo8vm18SfgImAtQES8DAwvZKPMrH2oqIq8l44mn9n2bhExRVrn03cVBWqPmbUjHTAm5i2f4LlQ0vakN8xLOh74sKCtMrN2oSNejucrn+B5DjAK+IKk94G3SW6UN7MiV8zBs8ExT0klwB4RcQiwOfCFiNjfz7ubGUBl5L/kQ9IPJc2Q9KqkOyV1kdRf0mRJMyXdJak8Lds53Z6V5vfLqeeiNP1NSYfnpA9N02ZJunB9zr3B4Jm+y/PcdH1FRCxbn4OZWcfSnBNGknoB3yfpsA0CSkkmp38NXBMRA4AlwOnpLqcDSyJiB+CatBySBqb77QIMBa5PH/IpJfls+hHAQGBEWjaTfGbbJ0o6X1IfSZtWL1kPaGYdRwFuVSoDukoqA7qRzK8cDNyT5t8KHJuuD0u3SfOHKJnZHgaMjYhPI+JtYBawV7rMiojZEbEGGMt63MOez5jnaenPc3LSAtgu60HNrGNoymy7pDOBM3OSRkXEqOqNiHhf0m+Bd4FVJJ/8eR5YGhHVd/jMBXql672A99J9KyR9DPRM0yflHCd3n/dqpee+t6NJ8nnCqH/Wys2sY2vKhFEaKEfVly9pE5KeYH9gKfBXkkvsz1VVvUs9efWl13WlnXnGq9HgKakLcDawf3qgfwE3RsTqrAc1s46hmWfbDwHejogFAJLuA74C9JBUlvY+ewMfpOXnAn2Auell/sbA4pz0arn71JfeZPmMed5GMvD6e+A6koHW27Me0Mw6jqrIf8nDu8A+krqlY5dDgNeAx4Hj0zIjgfvT9XHpNmn+YxERafrwdDa+PzAAmAJMBQaks/flJJNK47Keez5jnjtFxK45249LeinrAc2s42jOxy4jYrKke4AXSJ5inE5ymf8gMFbS5Wna6HSX0cDtkmaR9DiHp/XMkHQ3SeCtAM6JiEoASecCE0hm8sdExIys7c0neE6XtE9ETEoPvjfwTNYDmlnH0dw3yUfEJcAltZJnk8yU1y67muQVmXXVcwVwRR3p44Hx69/S/ILn3sCpkt5Nt/sCr1d/ktifIDYrXsX8hFE+wXNowVthZu2SXwzSAD+KaWb1qXTP08ys6Triezrz5eBpZpl5zNPMLIMi7ng6eJpZdu55mpllUFXV2i1oPQ6eZpZZVRFftzt4mllmayuLt+vp4GlmmbnnaWaWgSeMzMwycM/T1kvfXhtx+YWDa7Z7bbUBo/78Iht2L+eYwwew9JNPAbjh1ud5btr7HD54O046blBN+R36bcLI8x5g5uzF7LRDT372w/3pXF7Kc9PmcvUfp7T06RSdSy4+iAP225bFS1bx9RPvAuDKyw9j2217ALDhBuUsW76GEafcTVlpCT/76WC+sNPmlJWW8I+H3uTmW19ozea3KgdPWy/vvv8Jp34veadqSYl44Lav8+Sz73D0oQMYe/9r/OW+dV8ZOOGJ2Ux4YjYA22/bg6t+PoSZsxcD8OOz9+HK3z/Lq28s4JrLDmHf3Xvx3PPvt+wJFZkH/vEGd/31FX5xyZCatAsvfqRm/Yff/wrLV6wB4JAh21NeXso3TrqLLp3LuGfscB5+ZCYfflicH5Yt5uCZz5vkrQn22HVr3v/wE+YtWJFX+UO/uh0Tn0wCac9NutK9WzmvvrEAgPGPvcWB+/YtWFst8cKLH/JxenVQl0MP2YGHH5kJQBB07dKJ0lLRuXMpayuqWJEG1mK0trIq76WjcfBsZoce2J9Hnny7ZvuEo3fmz9cdw0/P248NNyj/XPlDDuxXU37znt1YsOizoDt/4Qo279mt8I22en15t61ZvHgl7733MQCPPjqbVavX8siD32T8uFO5/Y4X+aSBwNvRVVVF3ktH0+LBU9K3Gsg7U9I0SdPmv/tEC7aqeZSVlXDA3n147Ok5ANw3/g2OO+NeTvneOBYtWcn3T99znfK77LQZqz+tZPY7SwFQfd/8s1Zz+GEDanqdALvssgWVlcHhR93K0f/5Z04+cVd6bbNRK7awdTl4tqzL6suIiFERsUdE7LFF38Et2KTmse8evXjzrUUsXpp8WHTx0tVUVQURcP/DMxm442brlD/kwP41l+wA8xeuZPOe3Wu2t9isOwsWr2yZxtvnlJaKgw/ajkf+Oasm7YjDB/DcpHepqKxiyZJVvPTyPAbuvHkrtrJ1VUXkvXQ0BQmekl6uZ3kF2LIQx2wLDjtwu3Uu2Xtu0rVm/atf6VvTw4Sklzlk/35MfOqz8ouWrGLlqrXsslPyn/HIg7fnqUnvYq1j7z17M2fOEubP/2wo5cN5y9lzj14AdOlSxn8M2pI5OX+vxaaYe56Fmm3fEjgcWFIrXcCzBTpmq+rcuZS9vrQ1V1732emde9oeDNhuU4jgw/nLufL3z9XkfWnQVsxfuJIP5i1fp56r/vBccqtS51Kem/Y+z03zTHuh/eqXh7L7l7ehR48uPPTAqdw4air3P/A6hx06gIcfmbVO2bvveYVLf3Ywf71zOBKM+8cbzJy1qJVa3vo64kRQvhQF6E5LGg3cHBFP15H3l4g4sbE69jnqlo73q6oDW7PQwwvt0QuTz65rpD1vv/rnv/P+f/qTQ3Zcr2O1NQXpeUbE6Q3kNRo4zax96IiX4/nyTfJmlpmDp5lZBh1xFj1fDp5mlpl7nmZmGaytKN7ZdgdPM8vMl+1mZhn4st3MLINiDp5+q5KZZVaIxzMllUqaLukf6XZ/SZMlzZR0l6TyNL1zuj0rze+XU8dFafqbkg7PSR+aps2SdOH6nLuDp5llVlUZeS9NcB7wes72r4FrImIAySPf1Q+9URS9AAAGEUlEQVThnA4siYgdgGvSckgaCAwHdgGGAtenAbkU+ANwBDAQGJGWzcTB08wyq6yozHvJh6TewFHATem2gIOBe9IitwLHpuvD0m3S/CFp+WHA2Ij4NCLeBmYBe6XLrIiYHRFrgLFp2UwcPM0ss6Zctue+rzddzqyjyv8DfgxU3wPVE1gaERXp9lygV7reC3gPIM3/OC1fk15rn/rSM/GEkZll1pSxzIgYBYyqL1/S0cD8iHhe0uDq5LqqaiSvvvS6OouZZ7wcPM0ssyaOZTZmP+AYSUcCXYCNSHqiPSSVpb3L3sAHafm5QB9grqQyYGNgcU56tdx96ktvMl+2m1lmzTnbHhEXRUTviOhHMuHzWEScBDwOHJ8WGwncn66PS7dJ8x+L5B2b44Dh6Wx8f2AAMAWYCgxIZ+/L02OMy3ru7nmaWWb5TgStpwuAsZIuB6YDo9P00cDtkmaR9DiHA0TEDEl3A68BFcA5EVEJIOlcYAJQCoyJiHW/C94EDp5mllmhbpKPiCeAJ9L12SQz5bXLrAZOqGf/K4Ar6kgfD4xvjjY6eJpZZs085tmuOHiaWWbF/Himg6eZZebgaWaWQTh4mpk1XcVavwzZzKzJqqocPM3MmsxjnmZmGTh4mpll4OBpZpZB+CZ5M7Omq/Cnh83Mmi48225m1nS+bDczy8BPGJmZZeCep5lZBrG2RV6G3CY5eJpZZr5sNzPLwJftZmYZuOdpZpaF7/M0M2s6X7abmWVQ5Zchm5k1ncc8zcyyqHTP08ysydzzNDPLwBNGZmYZuOdpZpaBn203M8vAl+1mZhkU82V7SWs3wMzar6iMvJd8SBoq6U1JsyRdWODmrxf3PM0su2Z8tl1SKfAH4FBgLjBV0riIeK3ZDtKMHDzNLLNmfjxzL2BWRMwGkDQWGAa0yeCpiOIds2gtks6MiFGt3Q7Lj/++moekM4Ezc5JG5f65SjoeGBoRZ6TbpwB7R8S5LdvS/HjMs3Wc2XgRa0P899UMImJUROyRs9T+haS6dmuJtmXh4GlmbcVcoE/Odm/gg1ZqS6McPM2srZgKDJDUX1I5MBwY18ptqpcnjFqHx8/aF/99tYCIqJB0LjABKAXGRMSMVm5WvTxhZGaWgS/bzcwycPA0M8vAwbMFtadHzwwkjZE0X9Krrd0Wa3scPFtIzqNnRwADgRGSBrZuq6wRtwBDW7sR1jY5eLacmkfPImINUP3ombVREfEUsLi122Ftk4Nny+kFvJezPTdNM7N2yMGz5bSrR8/MrGEOni2nXT16ZmYNc/BsOe3q0TMza5iDZwuJiAqg+tGz14G72/KjZwaS7gSeA3aSNFfS6a3dJms7/HimmVkG7nmamWXg4GlmloGDp5lZBg6eZmYZOHiamWXg4FlEJPWQdHYB6/+mpOsaKXOppPObWO/y9WuZWfNz8CwuPYA6g2f61iczy5ODZ3G5Ethe0ouSfiNpsKTHJf0FeEVSv9x3V0o6X9Kl6fr2kh6W9Lykf0n6QkMHkvQ1SZMlTZf0T0lb5mTvKukxSTMlfTtnnx9JmirpZUmXNe+pmzUvfwCuuFwIDIqI3QAkDSZ5Vd6giHhbUr8G9h0FnBURMyXtDVwPHNxA+aeBfSIiJJ0B/Bj47zTvi8A+QHdguqQHgUHAgLQ9AsZJOjB9LZxZm+PgaVMi4u2GCkjaAPgK8Fep5uVQnRuptzdwl6StgXIg9xj3R8QqYJWkx0kC5v7AYcD0tMwGJMHUwdPaJAdPW5GzXsG6Qzld0p8lwNLqHmuefg9cHRHj0h7upTl5tZ8JDpLe5v9ExB+bcAyzVuMxz+KyDNiwgfyPgC0k9ZTUGTgaICI+Ad6WdAKAErs2cqyNgffT9ZG18oZJ6iKpJzCY5I1TE4DT0l4uknpJ2iL/UzNrWe55FpGIWCTpmXRS6CHgwVr5ayX9AphMcpn9Rk72ScANki4GOpF8RuSlBg53Kcll/vvAJKB/Tt6U9Nh9gV9GxAfAB5J2Bp5LhwaWAycD8zOerllB+a1KZmYZ+LLdzCwDB08zswwcPM3MMnDwNDPLwMHTzCwDB08zswwcPM3MMvj/+w3JkxnrlywAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y_test_np= np.asarray(y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mat = confusion_matrix(y_test_imb,log_prd_sm)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_imb,log_prd_sm).ravel()\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,cmap='RdYlBu_r',\n",
    "            xticklabels=['0','1'], yticklabels=['0','1'])\n",
    "plt.title(\"confusion Matrix\")\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "\n",
    "print(\"True positive rate:\",(tp/(tp+fn))*100)\n",
    "print(\"False positive rate:\",(fp/(tn+fp))*100)\n",
    "print(\"True negative rate:\",(tn/(tn+fp))*100)\n",
    "print(\"False negative rate:\",(fn/(tp+fn))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_sm = xgb.XGBClassifier()\n",
    "\n",
    "xgb_model_sm = xgb_model_sm.fit(X_train_smote, y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df = np.array(x_test_imb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232560, 31)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_prd_smote = xgb_model_sm.predict(x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the xgboost is {} 96.81759545923633\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98    229969\n",
      "           1       0.11      0.27      0.16      2591\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    232560\n",
      "   macro avg       0.55      0.62      0.57    232560\n",
      "weighted avg       0.98      0.97      0.97    232560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc = accuracy_score(y_test_imb, xgb_prd_smote) * 100\n",
    "print('\\nThe accuracy of the xgboost is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_imb,xgb_prd_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "opt_rf = RandomForestClassifier()\n",
    "opt_rf = opt_rf.fit(X_train_smote,y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_opt = opt_rf.predict(x_test_imb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the xgboost is {} 98.72205022359822\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    229969\n",
      "           1       0.17      0.04      0.06      2591\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    232560\n",
      "   macro avg       0.58      0.52      0.53    232560\n",
      "weighted avg       0.98      0.99      0.98    232560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc = accuracy_score(y_test_imb, prd_opt) * 100\n",
    "print('\\nThe accuracy of the xgboost is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_imb,prd_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "lr =LogisticRegression(C=100)\n",
    "sclf = StackingClassifier(classifiers=[optimal_nb_sm,optimal_lr_sm], meta_classifier=lr, use_probas=False)\n",
    "#sclf2 = StackingClassifier(classifiers=[sclf,optimal_nb_sm,optimal_lr_sm], meta_classifier=lr, use_probas=False)\n",
    "sclf2 = sclf2.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df = np.array(x_test_imb)\n",
    "\n",
    "x_test_df.shape\n",
    "\n",
    "sclf_prd_smote = sclf2.predict(x_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the xgboost is {} 62.59029927760578\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.63      0.77    229969\n",
      "           1       0.02      0.69      0.04      2591\n",
      "\n",
      "   micro avg       0.63      0.63      0.63    232560\n",
      "   macro avg       0.51      0.66      0.40    232560\n",
      "weighted avg       0.98      0.63      0.76    232560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test_imb, sclf_prd_smote) * 100\n",
    "print('\\nThe accuracy of the xgboost is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_imb,sclf_prd_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 68.73793901968352\n",
      "False positive rate: 37.47896455609234\n",
      "True negative rate: 62.52103544390766\n",
      "False negative rate: 31.26206098031648\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y_test_np= np.asarray(y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mat = confusion_matrix(y_test_imb,sclf_prd_smote)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_imb,sclf_prd_smote).ravel()\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,cmap='RdYlBu_r',\n",
    "            xticklabels=['0','1'], yticklabels=['0','1'])\n",
    "plt.title(\"confusion Matrix\")\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "\n",
    "print(\"True positive rate:\",(tp/(tp+fn))*100)\n",
    "print(\"False positive rate:\",(fp/(tn+fp))*100)\n",
    "print(\"True negative rate:\",(tn/(tn+fp))*100)\n",
    "print(\"False negative rate:\",(fn/(tp+fn))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anomoly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_fraction = 0.0112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "#from pyemma import msm # not available on Kaggle Kernel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(X)\n",
    "data = pd.DataFrame(np_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparamter tuning for isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:224: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IsolationForest(behaviour='old', bootstrap=False, contamination=0.0112,\n",
       "        max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "        n_jobs=None, random_state=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  IsolationForest(contamination = outliers_fraction)\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:418: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "processed_data['anomalyIF'] = pd.Series(model.predict(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    766515\n",
       "-1      8683\n",
       "Name: anomalyIF, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['anomalyIF'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    766515\n",
      "1      8683\n",
      "Name: anomalyIF, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "processed_data['anomalyIF'] = processed_data['anomalyIF'].map( {1: 0, -1: 1} )\n",
    "print(processed_data['anomalyIF'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = set(processed_data[processed_data['anomalyIF']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx = set(Y[Y==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx & true_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-8.640139498960897 > -10.215937415258388). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.031883481082110 > -16.638385236912818). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.140929537269344 > -18.632522602403476). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-25.049462060900328 > -88.910572423267951). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20.140376356116882 > -20.470266638553802). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-31.317039105354763 > -87.641772301824005). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.098083348791587 > -21.503313294127807). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.665536210758134 > -136.957070221108438). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.238335551865013 > -19.716969767331850). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-33.075618030931679 > -57.449930274621352). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.474617404895845 > -20.288230671636313). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-11.647338892205946 > -11.979247532352556). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-13.329359280362855 > -13.714329719648628). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-28.243873291816868 > -95.855117737494297). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-24.504621334016772 > -54.699308307728970). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.928569101223758 > -48.977700366124218). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.611568230765201 > -85.713239919085140). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.659545578333582 > -30.811611866051457). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-19.002057148469067 > -26.646916949299218). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.166008611547104 > -19.576553896948916). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.520560393113612 > -24.511226451440251). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.166008611547104 > -18.724373817507704). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-30.667547212104051 > -68.420430503972909). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-30.687332278976356 > -53.439464458842679). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-30.425816380499832 > -50.109911322707717). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-19.235775461492416 > -23.519809187714330). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-22.045838042834966 > -56.163069558412289). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.980063605284858 > -57.876517637174366). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.152830889793570 > -19.806835734574697). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-26.378953738013664 > -49.414592254597011). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.796418594854849 > -22.646700306545245). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-31.243239244487519 > -47.320367899222745). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-24.011936685685818 > -26.474035821308561). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-13.083903550448454 > -40.656182836626705). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-27.112433366345975 > -90.020706553743281). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-33.727413527220349 > -85.850650483498015). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-28.486665526224289 > -102.106977845921918). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-12.946016130020960 > -16.495146279856812). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.047010316292393 > -16.137854945137718). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-13.614936889661607 > -14.983169381277548). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.217897715538232 > -19.460943432850474). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-13.614936889661607 > -17.501763483555493). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.047010316292393 > -47.223009618168717). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.400473438646426 > -43.024425436696852). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-33.998134684618108 > -136.568574316714859). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-33.800234542183610 > -59.809391893208058). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21.432538053729211 > -93.503980420133217). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-60.568231030596834 > -100.624965283723355). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-34.683303027093167 > -54.167756783315653). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-24.794982205235364 > -60.313795418825748). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-26.245547046681889 > -31.920229283503662). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.076326797965628 > -23.369108089497075). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-24.200930022657012 > -88.547192146771991). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-24.960328838128127 > -83.675498871934806). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.425338272945915 > -20.350240641649698). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.561722610354241 > -21.779236718001044). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-13.028907443542700 > -14.510946597444864). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.425338272945915 > -17.527452056201120). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-22.494977675302213 > -53.308009866105429). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-24.530708082059519 > -47.477757795972458). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-12.977236811924962 > -14.089792853586083). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.064098098068508 > -18.192285907467127). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-51.138299668916368 > -123.335526545208779). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-12.780170628543985 > -14.670339863231803). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-28.482560175350770 > -98.059468714486783). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-22.450047460622304 > -51.124075693703404). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-22.450047460622304 > -53.260901254933081). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.051276611511401 > -17.637807796510643). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.463977095308232 > -50.772745963630101). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.157513872544051 > -22.447389252206399). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-38.433739119492728 > -62.272934494038907). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.638076884156810 > -25.426491984125747). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-30.217213888589527 > -49.092231432633909). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-9.781202097040556 > -10.642712822896058). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-22.414939238330163 > -31.325392566683693). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-25.520320931083479 > -25.587718619938904). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.250108100582519 > -17.622538319748912). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.255787630430957 > -15.961360483874344). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-13.406830411360632 > -17.758567106361230). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-10.944615653887769 > -13.596781795972909). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-33.144263181191725 > -141.893287946947765). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.207678859113127 > -20.573095125345340). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-25.634279821271051 > -26.117564429861059). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-25.853141907057481 > -50.614899789326287). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-25.853141907057481 > -33.124961727016739). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-29.854565985307033 > -95.527728364785034). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-31.345349268779152 > -54.730825689061732). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.866870953908297 > -97.937135593242687). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-11.147827977520064 > -17.171934581676283). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.492144107105979 > -17.465463127083858). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.492144107105979 > -19.758470334656305). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.072718114842516 > -20.034729837992089). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.396641716231173 > -24.396896428775072). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.451146658853606 > -94.234405329667908). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.053172551097969 > -17.956966811870529). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.696634197310527 > -53.956012536208334). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.053172551097969 > -18.625964677877661). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.696634197310527 > -46.721806747585866). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.696634197310527 > -18.245907931255786). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.696634197310527 > -17.808539058051359). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.793808127159467 > -14.870511975947784). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-27.694513438121874 > -61.451082478082952). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-36.044961551433992 > -64.558733384174531). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-37.236859597029515 > -58.346675728148035). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-64.558733384174531 > -93.676860267968607). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.379775604725793 > -22.669504991844654). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.660203142109303 > -22.674978281302067). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-19.263148579138793 > -62.133622961596487). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21.483496982868079 > -24.608055154543585). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-25.470371975767250 > -28.068108819240685). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.691262273009031 > -95.346990704992209). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-48.795136807136387 > -60.883232863194294). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-42.273513643096628 > -56.691262273009031). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-42.615974819700867 > -56.978296692571163). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.370468560137674 > -19.415408010307292). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-12.836690854983075 > -55.138998193925019). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-12.836690854983075 > -14.283731088528201). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-12.836690854983075 > -18.094482603994170). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-29.192828350869057 > -29.299727619627816). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.017552624034437 > -21.645655078003749). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.538493329272022 > -80.622433002873180). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.519655078640831 > -19.164066454024997). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.772723848494270 > -87.349234272859249). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.204567235889201 > -22.459651521063957). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.885414760990564 > -19.531848686337298). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.753087389144333 > -26.150175902608069). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-12.281066897503784 > -12.795347537547455). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-34.675317798744089 > -103.348472986747083). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21.804865384596333 > -50.452464370268991). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.560901518066181 > -43.904884230380667). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-21.637810191928619 > -21.784841528113496). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.927154692691495 > -46.177599607085945). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-26.520843880995638 > -62.218086456915316). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-22.197952450137308 > -60.267687521125453). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-18.056044930595029 > -18.767598194729899). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20.047358686087986 > -57.662252246776539). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-22.836954205932351 > -58.113509925925399). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-22.836954205932351 > -27.960604040444995). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-27.728841428771894 > -50.956432457832435). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.162872485516857 > -18.725525838794756). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-26.702558527786699 > -57.898751495758177). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-27.914685967164036 > -89.467174552520390). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.642440264475319 > -84.054199039559961). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.431210534280812 > -117.192811647572228). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-10.198367851382717 > -12.722259616292728). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-11.745813758186072 > -22.018703904031316). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-16.878374905113120 > -17.872475118659583). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.707041034736225 > -21.716196732267960). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.707041034736225 > -24.663484637658936). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-30.163990943172116 > -48.689916690078086). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.149401643657754 > -51.691680786297496). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.703420383432110 > -24.782522226335502). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20.956121497322965 > -22.045202529988849). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-29.744797481148581 > -57.428614600926856). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-35.355700435467348 > -66.115182344038303). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.797324416942580 > -94.097244693206747). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-19.831026402732682 > -20.930668498164042). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-26.071054028572959 > -48.917051222716417). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-15.030750801776207 > -48.674473875302894). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.395623170957705 > -19.131175322214691). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.473186820354513 > -22.698548162622593). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-24.521723265291708 > -29.737539750390994). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-20.415354846237481 > -25.764110077918378). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-23.100238509819906 > -50.408700869358078). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-17.525715719636182 > -19.444012652685522). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-27.705763640232792 > -85.179153610064716). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.226690833249435 > -43.397814198938157). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-14.845362543867829 > -52.185488194983328). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-13.307475560703596 > -40.446482874138326). You may want to try with a higher value of support_fraction (current value: 0.503).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-92.287011500825741 > -92.639827623873003). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.293285433029652 > -90.083431682869985). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.892490312212729 > -76.510049728387074). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.865773831206745 > -79.708875737176086). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.348176415137459 > -90.263978502354746). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.550101570340146 > -86.316361811395197). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.949940244975423 > -83.102851171569014). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.428476582086148 > -87.802048104902866). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-60.813574571940684 > -89.928844551628373). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.314554912976831 > -86.037350842942033). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.631931587510962 > -84.528291458110331). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.452138639192320 > -84.472248373886487). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.963833402796830 > -84.970578525984109). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.986846468204142 > -89.244508528618496). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.299844519908575 > -91.206771421537610). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.555072074274364 > -90.974430248278168). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.116576574160085 > -78.231766582109699). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.542532143414491 > -90.497087873853346). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.050574380990462 > -86.597085877274083). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.303758323168452 > -89.964690083859338). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.912998893633223 > -76.295601348272044). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-54.152846878622050 > -70.163633344873688). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.079639690538592 > -89.253389083655662). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.855807331618777 > -84.312011941874715). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.250225806645844 > -92.609399562717414). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.590861448304175 > -84.303920322617259). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-53.041746980539337 > -73.851524200750703). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-54.125097380198206 > -76.320269412196382). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.443523257687922 > -78.547970385954997). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.577681355593725 > -87.976077458293105). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.827302206927030 > -87.925373304532172). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.311937981725173 > -57.997192962851102). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.952747042150378 > -92.041210439380691). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-60.007875451199986 > -92.743029415575364). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.539864592209902 > -86.876326145025047). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.644225587939601 > -86.300450967924007). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.012491159365219 > -85.385075285086657). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-91.230438251355167 > -92.411784159959566). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.209603521208429 > -92.867950369260370). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.511266916076792 > -85.835070539071950). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.436263232974063 > -86.640435815775334). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.139610871338206 > -88.418447196325047). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.881646930701088 > -87.392862830086898). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.755349752014155 > -88.573815418361164). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.827743775457890 > -88.269835021128202). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-53.970393851587211 > -71.670917457355088). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.747178173751081 > -86.667805334524331). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.321957028095611 > -88.079903169315401). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.287814621598997 > -87.435928322704825). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.652827902183887 > -88.409488168667650). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.711126601832163 > -73.466780602542912). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.358728932416533 > -84.423278822369639). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.542065197235928 > -87.042333457964745). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.046064023356969 > -85.106448963741059). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-92.893328104234598 > -95.872630911144867). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.861903758545665 > -86.653444474682374). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.809421110879292 > -81.724417883467623). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.441414424074054 > -76.181677884255464). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.473669249595453 > -88.327833018836145). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.672869088477377 > -87.586272004774770). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.503775605230153 > -77.327821290211290). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.421611846471230 > -89.862129574339008). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-60.225412977769466 > -89.789376601744664). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.247147840007941 > -87.425731592958414). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-60.313601523435985 > -84.298193331629108). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.605481273651186 > -82.035651988074676). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.838945920255291 > -85.328113013285332). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.846520728828942 > -74.934411642793975). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.910738232119954 > -83.795670915837349). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.792547879641255 > -77.017280927979414). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.847781392472577 > -84.669389145425370). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-60.222933560151276 > -88.065806846359919). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.940291072169728 > -83.413347878817163). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.268081384527832 > -75.216808057071233). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.412198222224021 > -57.532267764607184). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.610992473520454 > -77.066066841598712). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.250420257325437 > -75.851359515250380). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.153040488329268 > -78.780930691688681). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.527520605484014 > -57.730554619281349). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.581356385531585 > -91.624355203486431). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.849598520474693 > -79.804004578234512). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.063735803900535 > -84.520638989534149). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.830405540193084 > -88.577397788734430). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.287544732790451 > -57.497055118936942). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.531458714502790 > -78.739437050344407). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.307205006633986 > -82.481912395108282). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.450154660473700 > -83.071124850410740). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.442402190633452 > -83.423532618233637). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.588217769887528 > -84.800541667722456). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.622276415886056 > -60.383547934385753). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.267565756213941 > -88.838284359091659). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.417779812391871 > -85.687421549924196). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.947751739471407 > -85.200612430897792). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.319270997365706 > -87.982616631268414). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.219450132340214 > -86.976155749686612). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.842664357613714 > -88.079689578414786). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.617236284692837 > -89.921207670065428). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.079813687177683 > -87.568541586408585). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.007736163673130 > -86.106551302372566). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-53.856469966661784 > -71.398046925825511). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-52.304621777122527 > -70.972056090901603). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.080320610487675 > -71.904580251164916). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.671739840383331 > -79.852841112694193). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.020030021627598 > -89.683563250225546). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.860228963459782 > -75.438761598986062). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.131706856214137 > -78.147536522747345). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.170638298473257 > -88.662438981772652). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.538917032396952 > -89.341252667696281). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.631688885802461 > -88.694065484420122). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.443425906919451 > -89.991432890453353). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.972597223433617 > -90.316538577973461). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.335022715196047 > -84.415612717570212). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-92.528655983346923 > -92.878407775690022). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.586639584971813 > -77.609415074814521). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.294798628650206 > -83.385733137543099). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.532431667160637 > -89.438239507289438). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.269434355080065 > -84.677389026514817). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.952805898117063 > -72.688516698887923). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.466048245906215 > -90.612112899111622). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.936207521042874 > -79.960046401134065). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-54.742384038873794 > -73.615212484822422). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.496724705413151 > -87.311062116713998). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.820515863214013 > -91.620309211220672). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.360822944867557 > -74.688826821294398). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.507535695407348 > -90.297365122105191). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.876377369324658 > -88.178483650037307). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.469216151729604 > -86.010979663119230). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.177576008235462 > -83.745347735443943). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-55.816330111145760 > -76.698438336380761). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.335455738643788 > -88.972822408151089). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-54.306286869388074 > -73.097591519539463). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.033345456835832 > -88.980658628881741). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.749905509494127 > -89.280143760619296). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.500058563042018 > -76.161860775603643). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.528296941183328 > -88.570255714224828). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.856862066380366 > -91.249031412662333). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.292999019885812 > -82.661957486698796). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.438105290044831 > -82.677718505189944). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.275766068041747 > -58.633537816122889). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.198382026251970 > -87.509992716000639). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.331243832283441 > -85.111450870642443). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-58.622810827720791 > -58.690148431272540). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-56.114365186264379 > -73.984892710341541). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-53.650255160594838 > -72.210873827569515). You may want to try with a higher value of support_fraction (current value: 0.501).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.995017014110864 > -69.265345656361831). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-59.425472418710484 > -71.063454258118469). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-63.014853562778690 > -67.683561011679046). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-62.934797356276412 > -67.651283658686367). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-61.366873526891666 > -68.617167065731209). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-57.985617521168876 > -69.256543772027030). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-62.556449006049590 > -69.515646401605665). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-64.255891620775031 > -67.367692948744846). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\covariance\\robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-62.883434359600635 > -67.648813615655641). You may want to try with a higher value of support_fraction (current value: 0.500).\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "envelope.fit(X)\n",
    "processed_data['anomalyEE'] = envelope.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['anomalyEE'] = processed_data['anomalyEE'].map( {1: 0, -1: 1} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    766515\n",
       "1      8683\n",
       "Name: anomalyEE, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['anomalyEE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "anmly1_set = set(processed_data[processed_data['anomalyEE']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "anml25 = set(processed_data[processed_data['anomalyIF']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx = set(Y[Y==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anmly1_set & true_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anmly1_set & anml25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anmly1_set & anml25 & true_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_am, X_test_am, y_train_am, y_test_am = train_test_split(processed_data, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int32, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scal = MinMaxScaler()\n",
    "X_train_am = scal.fit_transform(X_train_am)\n",
    "X_test_am = scal.transform(X_test_am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 96.92853457172342\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98    229969\n",
      "           1       0.11      0.25      0.15      2591\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    232560\n",
      "   macro avg       0.55      0.61      0.57    232560\n",
      "weighted avg       0.98      0.97      0.98    232560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "optimal_nb_sm = BernoulliNB()\n",
    "lp = optimal_nb_sm.fit(X_train_am, y_train_am)\n",
    "\n",
    "nb_prd_am = optimal_nb_sm.predict(X_test_am)\n",
    "\n",
    "acc = accuracy_score(y_test_am, nb_prd_am) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_am,nb_prd_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y_test_np= np.asarray(y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mat = confusion_matrix(y_test_am,nb_prd_am)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_am,nb_prd_am).ravel()\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,cmap='RdYlBu_r',\n",
    "            xticklabels=['0','1'], yticklabels=['0','1'])\n",
    "plt.title(\"confusion Matrix\")\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 24.93245851022771\n",
      "False positive rate: 2.260304649757141\n",
      "True negative rate: 97.73969535024285\n",
      "False negative rate: 75.0675414897723\n"
     ]
    }
   ],
   "source": [
    "print(\"True positive rate:\",(tp/(tp+fn))*100)\n",
    "print(\"False positive rate:\",(fp/(tn+fp))*100)\n",
    "print(\"True negative rate:\",(tn/(tn+fp))*100)\n",
    "print(\"False negative rate:\",(fn/(tp+fn))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 86.30589955280358\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.93    229969\n",
      "           1       0.06      0.72      0.10      2591\n",
      "\n",
      "   micro avg       0.86      0.86      0.86    232560\n",
      "   macro avg       0.53      0.79      0.52    232560\n",
      "weighted avg       0.99      0.86      0.92    232560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimal_lr_sm = LogisticRegression(class_weight='balanced')\n",
    "lp = optimal_lr_sm.fit(X_train_am, y_train_am)\n",
    "\n",
    "log_prd_am = optimal_lr_sm.predict(X_test_am)\n",
    "\n",
    "acc = accuracy_score(y_test_am, log_prd_am) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_am,log_prd_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y_test_np= np.asarray(y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mat = confusion_matrix(y_test_am,log_prd_am)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_am,log_prd_am).ravel()\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,cmap='RdYlBu_r',\n",
    "            xticklabels=['0','1'], yticklabels=['0','1'])\n",
    "plt.title(\"confusion Matrix\")\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 71.67116943265148\n",
      "False positive rate: 13.529214807213148\n",
      "True negative rate: 86.47078519278686\n",
      "False negative rate: 28.32883056734851\n"
     ]
    }
   ],
   "source": [
    "print(\"True positive rate:\",(tp/(tp+fn))*100)\n",
    "print(\"False positive rate:\",(fp/(tn+fp))*100)\n",
    "print(\"True negative rate:\",(tn/(tn+fp))*100)\n",
    "print(\"False negative rate:\",(fn/(tp+fn))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters_rscv = {'C': [10**-4, 10**-2,10**-1, 10**0, 10**1,10**2,10**3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:271: UserWarning: The total space of parameters 7 is smaller than n_iter=10. Running 7 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "0.03255484784147204\n"
     ]
    }
   ],
   "source": [
    "model = RandomizedSearchCV(LogisticRegression(), tuned_parameters_rscv, scoring = 'f1', cv=5)\n",
    "model.fit(X_train_am, y_train_am)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(model.score(X_test_am, y_test_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAYESH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the Logistic Regression is {} 85.58694530443756\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92    229969\n",
      "           1       0.06      0.75      0.10      2591\n",
      "\n",
      "   micro avg       0.86      0.86      0.86    232560\n",
      "   macro avg       0.53      0.80      0.51    232560\n",
      "weighted avg       0.99      0.86      0.91    232560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimal_lr_sm = LogisticRegression(C=1000,class_weight='balanced')\n",
    "lp = optimal_lr_sm.fit(X_train_am, y_train_am)\n",
    "\n",
    "log_prd_am = optimal_lr_sm.predict(X_test_am)\n",
    "\n",
    "acc = accuracy_score(y_test_am, log_prd_am) * 100\n",
    "print('\\nThe accuracy of the Logistic Regression is {}' , acc)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test_am,log_prd_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=10000, compute_labels=True, init='k-means++',\n",
       "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=10,\n",
       "        n_init=3, random_state=42, reassignment_ratio=0.01, tol=0.0,\n",
       "        verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeanModel = MiniBatchKMeans(n_clusters=10,batch_size=10000,random_state=42)\n",
    "kmeanModel.fit(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "K = range(20,40)\n",
    "for k in K:\n",
    "    print(k)\n",
    "    kmeanModel = MiniBatchKMeans(n_clusters=k,batch_size=10000,random_state=42)\n",
    "    kmeanModel.fit(X)\n",
    "    cost.append(sum(np.min(pairwise_distances(X=X, Y=kmeanModel.cluster_centers_, metric='euclidean'), axis=1))/X.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(K, cost, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.drop(columns='cluster',inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_mini_batch = MiniBatchKMeans(n_clusters=25, batch_size=20000,random_state=0).fit(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['cluster']=kmeans_mini_batch.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8590\n",
       "4      27\n",
       "5       3\n",
       "2       3\n",
       "1       1\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.iloc[Y[Y==1].index]['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    767148\n",
       "4      5159\n",
       "2      1549\n",
       "5       740\n",
       "1       318\n",
       "6       184\n",
       "3        80\n",
       "7        20\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 28)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_mini_batch.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730676"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictlist[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(775198, 29)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {i: np.where(kmeans_mini_batch.labels_ == i)[0] for i in range(kmeans_mini_batch.n_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictlist = []\n",
    "for key, value in mydict.items():\n",
    "    temp = [key,value]\n",
    "    dictlist.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['cluster']=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_index = set(Y[Y==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
